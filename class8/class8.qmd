---
title: "class8: Breast Cancer mini project"
name: "solomon kim"
format: pdf
---

## About 

In Today's lab we will work with fine needle aspiration (FNA) of a breast mass from the university of Wisconsin. 


## Data Import 

```{r}
fna.data <- read.csv("WisconsinCancer.csv")
wisc.df <- data.frame(fna.data, row.names=1)
head(wisc.df)
```

> Q1. How many patients/individual samples are in this dataset? 

```{r}
nrow(wisc.df)
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}

sum(wisc.df$diagnosis == "M")

table(wisc.df$diagnosis)

```
> Q3. How many variables/features in the data are suffixd with _mean

```{r}
ncol(wisc.df)
colnames(wisc.df)
```
```{r}
inds <- grep("_mean", colnames(wisc.df), value=T )
length(inds)
inds
```


```{r}
grep("_mean",colnames(wisc.df), value=T) 
```



## Initial Analysis 

Before Analysis I want to take out the expert diafnoses column (a.k.a the anaswer) from out dataset. 

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
head(diagnosis)
```

```{r}
wisc.data <- wisc.df[,-1]
#wisc.data
```

## Clustering 

We can try a kmeans() clustering first 

```{r}
km <- kmeans(wisc.data, centers=2)
```
```{r}
table(km$cluster)
```

Cross-table


```{r}
table(km$cluster, diagnosis)
```

lets try `hclut()` the key input ewquired for `hclust()` is a distance matrix as produced by the `dist()` function. 

```{r}
hc <- hclust(dist(wisc.data))

```

I can make a tree like figure 

```{r}
plot(hc)
```



## PCA

Do we need to scale the data? 

We can look at the sd of each column (original variable)

```{r}
apply(wisc.data,2,sd,)
```

Yes we need to scale. We will run `prcomp()` with `scale=TRUE`

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

Generate our main PCA plot (score plot, PC1 vs PC2 plot)

```{r}
library(ggplot2)

res <- as.data.frame(wisc.pr$x)

```

```{r}
ggplot(res) + aes(PC1, PC2, col=diagnosis) + geom_point()
```

## Combining Methods

Clustering on PCA results

Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Wardâ€™s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.

```{r}
d <- dist(wisc.pr$x[,1:3])
hc <- hclust(d, method="ward.D2")
plot(hc)

```

To get my clustering result/membership vector I eed to "cut" the tree with the `cutree()` function. 


> Q. How many patients are in each cluster group? 


```{r}
grps <- cutree(hc, k=2)
table(grps)
```

```{r}
plot(res$PC1, res$PC2, col=grps)
```


## Predicition

We can use our PCA result (model) to do predicitions, that is take new unseen data and project it onto our new PC variables 



```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```


```{r}
plot(res$PC1, res$PC2, col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], labels=c(1,2), col="white")
```


## Summary 

Principal Component Analysis (PCA) is a super useful method for analyzing large datasets. It works by finding new variables (PCs) that capture the most variance from the original variables in your dataset. 






